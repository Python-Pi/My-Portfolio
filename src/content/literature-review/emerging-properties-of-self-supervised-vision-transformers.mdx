---
title: "Emerging Properties in Self-Supervised Vision Transformers"
description: "A Meta AI research paper exploring the capabilities of vision transformers trained with self-supervised learning."
publishDate: 2026-01-15
authors:
  - Mathilde Caron
  - Hugo Touvron
  - Ishan Misra
  - Hervé Jégou
  - and others
tags:
  - "Machine Learning"
  - "self-supervised-learning"
  - "vision-transformers"
link: "https://openaccess.thecvf.com/content/ICCV2021/papers/Caron_Emerging_Properties_in_Self-Supervised_Vision_Transformers_ICCV_2021_paper.pdf"
draft: false
status: "published"
---

import ScrollLink from '../../components/ScrollLink.astro';

This paper is about exploring **self supervised learning in vision transformers (ViTs)**. The authors investigate how ViTs trained with self-supervised methods compare to standard methods like convolutional neural networks (CNNs) and supervised learning with ViTs. 
The key findings of the paper include:
- ViTs trained with self-supervised learning can learn rich semantic features compared to CNNs or supervised ViTs.
- These features are also excellent K-NN classifiers, demonstrating strong performance on image classification tasks.
- They also developed a method called DINO (self-**DI**stillation with **NO** labels) which allows ViTs to learn from unlabeled data effectively.

## Introduction 

$$\qquad$$ Tranformers have emerged as a powerful architecture in computer vision but in the early days of it's adoption mostly with supervised learning,
the improvements offered by transformers over convolutional neural networks (CNNs) were modest at best even though they need significantly more resources to train.

One of the main reason transformers dominated the Natural Language Processing (NLP) field was due to the use of self-supervised methods,
in the form of close procedure in BERT or language modeling in GPT. These self-supervised pretraining objectives use the words
in a sentence to create pretext tasks that provide a richer learning signal than the supervised objective of predicting a single label per sentence.

Similarly, in images, image level supervision often reduces the rich visual information contained in an image to a single concept selected from a predefined set of a few thousand categories of objects. 
That's why self supervised learning with convnets was tried before which is the inspiration for this paper to explore self supervised learning with vision transformers.

## Related Work

$$\qquad$$ A large amount of work on self-supervised learning focusess on **discriminative approaches** that learn representations by comparing different views of the same image.
These are coined as **instance classification** methods where each image is treated as its own class and the model learns to discriminate between different images.
During training, the model is asked to distinguish one image from all others, not by memorizing pixel values, but by learning representations that remain consistent under
various data augmentations such as cropping, color jittering, or blurring. Two augmented views of the same image are considered positives, while views from different images 
are treated as negatives. The use of strong augmentations is crucial here, as it defines the invariances the model should learn and prevents trivial solutions.

This forces the model to learn meaningful features that capture the essence of the images rather than just memorizing them.
Although instance classification has proven effective at learning rich visual features, it comes with important limitations. 
Explicitly discriminating between all images does not align perfectly with semantic understanding: images of the same object category 
(for example, two different dogs) are still treated as distinct and are actively pushed apart in feature space. As a result, 
semantic grouping emerges only indirectly, as a side effect of learning to solve the discrimination task.

Additionally, this approach poses scalability and efficiency challenges.

### Self Learning 

$$\qquad$$ Self learning is a strategy where the model tries the propogate information from a set of labelled data
to a larger set of unlabelled data. The model is first trained on the labelled data and then used
to predict the labels for the unlabelled data (also called psuedo-labelling). The model is then retrained on the combined dataset of labelled and pseudo-labelled data.
Doing this iteratively over time helps propagate the label information from the labeled subset to the unlabeled majority, improving the quality of learned features.

The propagation can be done in two ways:
- **Hard propagation**: The model assigns a single pseudo-label to each unlabeled example based on its highest-confidence prediction.
- **Soft propagation**: The model assigns a probability distribution over all possible labels to each unlabeled example. Soft labels carry richer information, such as uncertainty and inter-class similarity, and tend to produce better and more stable learning outcomes.

### Knowledge Distillation 

$$\qquad$$ When soft labels are used, self learning becomes closely related to Knowledge Distillation, where a large pre-trained model (the teacher)
is used to generate soft labels for training a smaller model (the student). Knowledge distillation has been shown to be effective for model compression.

Both of the above mechanisms are a way to transfer information from a stronger signal to a weaker one. 

### Knowledge Distillation with no labels 

$$\qquad$$ The idea of knowledge distallation can be pushed even further by removing the need for any labels at all. In this case, 
the teacher model itself emerges during training. The teacher model is slowly updated as an exponential moving average of the student model.

The model learns by making different views (augmentations) of the input, the teacher's predictions are used as soft labels for the student, 
even though neither models has access to ground labels. The teacher is dynamic in this case compared to traditional knowledge distillation where the teacher is pre-trained and fixed.

## Approach

### SSL with Knowledge Distillation

Knowledge distillation is a methodology where we train a student model  $$ g_s $$ to match the output of the teacher model $$ g_t $$. 
Given an input image $$ x $$, both the networks output a probability distribution over $$ K $$ dimensions (classes) denoted by $$ P_s $$ and $$ P_t $$. 
The probability $$ P $$ of network $$ g $$ is obtained by applying a softmax on the output logits of the network.

```math
P_s(x)^{(i)} =
\frac{
  \exp\!\left( g_{s}(x)^{(i)} \,/\, \tau_s \right)
}{
  \sum_{k=1}^{K}
  \exp\!\left( g_{s}(x)^{(k)} \,/\, \tau_s \right)
}
```
where $$ \tau_s > 0 $$ is the temperature parameter that controls the smoothness/sharpness of the output distribution. 
Higher values of $$ \tau $$ produce softer probability distributions over classes whereas lower values produce sharper distributions.


Given a fixed teacher network $$ g_t $$, the student network $$ g_s $$ is trained to minimize the <ScrollLink target="key-concepts"> cross entropy loss </ScrollLink> between the two distributions:

```math
\min_{s} \; H\!\left(P_t(x),\, P_s(x)\right)
```
where $$ H(q, p) = - \sum_{y} q(y) \log p(y) $$ 

In order to train both the student and teacher networks without any labels, we construct different distorted views 
of an image with multi-crop strategy. More precisely, we generate a set $$ \mathcal{V} $$  of different views which contain two 
<ScrollLink target="key-concepts">global views</ScrollLink> and several other <ScrollLink target="key-concepts">local views</ScrollLink> for each image. 
All the crops are passed through the student network whereas only the global crops are passed through the teacher network. The loss minimization function
for this can be expressed as:

```math
\min_{s}
\sum_{x \in \{x_1^g, x_2^g\}}
\sum_{\substack{x' \in \mathcal{V} \\ x' \neq x}}
H\!\left(P_t(x), P_s(x')\right)
```
where $$ x_1^g, x_2^g $$ are the two global crops of the image $$ x $$ and $$ \mathcal{V} $$ is the set of all views of the image.

*We used cross entropy loss between the teacher and student and tried to minimize it. 
It is done so that the student learns to predict the same (or similar) output distribution as that of teacher 
for different views of the same image.*

### Teacher Network Updating

In contrast to traditional knowledge distillation where the teacher network is pre-trained and fixed, the teacher network in this cases 
is dynamic and is updated based on the student network. The researchers found that directly copying the student weights to the 
teacher fails to converge. They introduced a method called ** Exponential Moving Average (EMA)** to update the teacher network. 
The update rule is given by:

```math

t \leftarrow \lambda \, t + (1 - \lambda) \, s
```
where $$ t $$ and $$ s $$ are the weights of the teacher and student networks respectively and $$ \lambda \in (0.99, 1) $$

### Network architecture

The neural network ($$g$$) is composed of a feature extractor ($$f$$) and a projection head ($$h$$):

```math
g(x) = h(f(x))
```

The feature extractor $$ f $$ is a Vision Transformer (ViT) and the projection head $$h$$ used in this paper is a 3-layer 
Multi-Layer Perceptron (MLP) followed by $$l_2$$ normalization and a weight-normalized fully connected layer producing 
$$K$$-dimensional outputs. A <ScrollLink target="key-concepts">predictor</ScrollLink> is not used in this work unlike other self-supervised learning methods.

Of particular interest, we note that unlike standard convnets, ViT architectures do not use batch normalizations (BN) by default. Therefore,
when applying DINO to ViT we do not use any BN also in the projection heads, making the system entirely *BN-free*.

### Avoiding Collapse

[See Mini Blog: What is Collapse in Self-Supervised Learning?](/mini-blogs/what-is-collapse-in-self-supervised-learning)

Teacher model produces logits over a set of dimensions. Collapse in this context can happen in two main ways:
- A single dimension dominating the output, leading the student model to always predict that dimension.
- The model producing uniform or near-uniform distributions across all dimensions, leading to a lack of meaningful information.

This paper use **Centering + Sharpening** to avoid collapse in the student-teacher framework. Each of 
these techniques tackle one of the above issues and they work in opposite directions. Hence, using them together
helps maintain a balance and prevents collapse effectively.

#### Centering

Centering addresses the first failure mode by maintaining a running exponential-moving-average of the 
teacher's batch-mean output and subtracting this mean from future teacher logits. 
Intuitively, this removes global bias and prevents any single dimension from being consistently large across the dataset, 
forcing the model to use all dimensions more evenly. However, centering alone pushes the system toward the second failure mode. 

```math
g_t (x) \leftarrow g_t (x) - c
```
Here, $$ c $$ is the running mean of the teacher outputs. This mean is updated after every batch as follows:

```math
c \leftarrow m \, c + (1 - m) \, \frac{1}{B} \sum_{i=1}^{B} g_t(x_i)
```
where $$ m \in (0, 1) $$ is the momentum parameter, $$ B $$ is the batch size, and $$ x_i $$ are the samples in the current batch.

#### Sharpening

Sharpening counteracts this by applying a low-temperature softmax to the teacher outputs, 
which amplifies small differences between logits and produces confident, peaked distributions, preventing uniformity but, 
if used alone, risking domination by the same dimension everywhere.

*Think of analogy in image processing, where centering is like adjusting brightness to avoid overly bright areas, 
while sharpening is like enhancing contrast to ensure details stand out.*

## Results

Here are some of the impressive results achieved by DINO with ViTs on various datasets without any finetuning:
<img
  src="/emerging-properties-of-self-supervised-vits/img2.PNG"
  alt="Image Retrieval"
  style={{
    display: "block",
    margin: "0 auto",
    width: "100%",
    maxWidth: "600px",
    height: "auto",
    marginBottom: "10px"
  }}
/>

<img
  src="/emerging-properties-of-self-supervised-vits/img3.PNG"
  alt="Copy Detection"
  style={{
    display: "block",
    margin: "0 auto",
    width: "100%",
    maxWidth: "600px",
    height: "auto",
    marginBottom: "10px"
  }}
/>

<img 
  src="/emerging-properties-of-self-supervised-vits/img4.PNG"
  alt="Video Object Segmentation"
  style={{
    display: "block",
    margin: "0 auto",
    width: "100%",
    maxWidth: "600px",
    height: "auto",
    marginBottom: "10px"
  }}
/>

<img 
  src="/emerging-properties-of-self-supervised-vits/img5.PNG"
  alt="Attention Maps"
  style={{
    display: "block",
    margin: "0 auto",
    width: "100%",
    maxWidth: "600px",
    height: "auto",
    marginBottom: "10px"
  }}
/>

<img 
  src="/emerging-properties-of-self-supervised-vits/img6.PNG"
  alt="Attention Maps"
  style={{
    display: "block",
    margin: "0 auto",
    width: "100%",
    maxWidth: "600px",
    height: "auto",
    marginBottom: "10px"
  }}
/>




## Conclusion

The paper has shown the potential of self-supervised pretraining a standard ViT model, achieving performance that are
comparable with the best convnets specifically designed for this setting.

The paper has identified several interesting properties of self-supervised ViTs that do not emerge from supervised ViTs or convnets:
- Self-supervised ViT features explicitly contain the scene layout and, in particular, object boundaries. This information is directly accessible in the self-attention modules of the last block.
![Object boundaries](/emerging-properties-of-self-supervised-vits/img1.PNG)
- Self-supervised ViT features perform particularly well with a basic nearest neighbors classifier (k-NN) without any finetuning, 
linear classifier nor data augmentation, achieving 78.3% top-1 accuracy on ImageNet.

*Note: Top-1 accuracy measures how often a model’s single most confident prediction is correct*

- Importance of using smaller patches (8x8/16x16) for self-supervised ViTs to learn better features.


## Key Concepts
**Self Supervised Learning**: A type of machine learning where the model learns from the data itself without explicit labels.

**Vision Transformers (ViTs)**: A type of neural network architecture that applies the transformer model, originally designed for natural language processing, to image data.

**Momentum Encoder**: 
A momentum encoder is a method used in self-supervised learning to make training more stable and reliable.

Instead of training only one neural network, we use two versions of the same model:
- **Student (online model)**:
  Learns quickly and is updated normally using backpropagation.
- **Teacher (momentum encoder)**:
  Learns slowly and is not trained directly.
  Instead, it is updated by taking a slow moving average of the student’s weights.

In simple terms: The student learns fast, and the teacher slowly follows the student.

**Multi-crop augmentation**: A data augmentation technique where multiple crops of different sizes are taken from the same image to create diverse training samples.

**[CLS] token**: In transformer models, The [CLS] token acts like a summary node that gathers information from all other tokens through self-attention.

**Softmax Function**: A mathematical function that converts a vectors of numners into probabilities that sum to 1.

```math
\text{Softmax}(z_i) = \frac{e^{z_i}}{\sum_{j} e^{z_j}}
```

**Cross Entropy Loss**: A loss function used in classification tasks to measure the difference between two probability distributions.
Consider the two distributions $$q(y)$$ and $$p(y)$$ . The cross-entropy loss is defined as:

```math
H(q, p) = - \sum_{y} q(y) \log p(y)
```
A lower cross-entropy loss indicates that both the distributions are similar. Also, the negative sign is added because the log probabilities are negative.

**Global Crop**: A large crop of the image that captures most of the content(60%-100%), used to provide context during training. 

**Local Crop**: A smaller crop of the image that focuses on specific details(5%-50%), used to encourage the model to learn fine-grained features.

$$\rightarrow$$ [Why Transformers Don't Use Batch Normalization](/mini-blogs/why-transformers-dont-use-batch-normalization)

**Predicator**: In context of self-distillation, a predicator is often a small MLP placed after the projection head of the student network. 
Predictors were introduced to prevent representation collapse (everything mapping to the same vector) without using negative samples. 


> Student: $$f \rightarrow h \rightarrow $$ predicator

> Teacher: $$f \rightarrow h$$

DINO notably does not use a predicator on the student network yet avoids collapse through other techniques like centering and sharpening.